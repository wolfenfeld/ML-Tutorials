---
layout: post
title: Not All Who Wander Are Lost
description: |
  A Quick Start Guide to Multi Armed Bandits
image: /assets/img/Bandit-post/comics-doctor-octopus-day-off-shop.jpeg
noindex: true
---

# Not All Who Wander Are Lost

Decisions are hard, they have always been. And when you finally find something you like, there is always that thought, in the back of your head - "can I find something better?".

One of the challenges that arise in _Reinforcement Learning_ is the _Exploration - Exploitation Dilemma_, do we choose - **exploitation**, where we make the best decision given current knowledge or **exploration**, where we gather more knowledge that might lead us to better decisions in the future.

In this post we will introduce the Multi Armed Bandits (MAB) problem, show two algorithms that solve this problem, and run a simulation.

# Multi Armed Bandits

The multi-armed bandit problem, or bandit problem for short, is one of the simplest instances of the sequential decision making problem, in which a learner needs to select options from a given set of alternatives repeatedly in an online manner. The name comes from the gambling world in which a gambler decides from a row of slot machines, sometimes known as "one-armed bandits".

# Algorithms

Before we get to the good stuff we need to understand what we are trying to achieve. As described the multi armed bandit problem is a sequential decision making problem.

The "game" is set like this: in each round the agent decides on an action (arm), pulls the arm and receives a reward $${u_r}$$. The arm belongs to an finite set $${X}$$.

Each arm $${x\in X}$$, is associated with a probability distribution over [0, 1], with expectation $${\mu_x}$$. We will assume the existence of a unique best arm: $${x^* =argmax_x(\mu_x)}$$.

The object of the game is to minimize the cumulative regret, that is defined as:

![Regret Definition][regret]

The cumulative regret shows the difference between the utility the player could have acquired if he played the best arm and the sum of utilities actually acquired.

Assuming we have $${K}$$ arms, and $${T}$$ rounds to play, and we decide to play each arm $${T/K}$$ times - The regret that we would endure is linear with $${T}$$. But we can do better...

## UCB

## Thompson Sampling

# Implementation

# Simulation

# Conclusion

[regret]: /assets/img/MAB-post/regret.png
[x_in_x]: /assets/img/MAB-post/x_in_X.png
