---
layout: post
title: Not All Who Wander Are Lost
description: |
  A Quick Start Guide to Multi Armed Bandits
image: /assets/img/Bandit-post/comics-doctor-octopus-day-off-shop.jpeg
noindex: true
---

# Not All Who Wander Are Lost

Decisions are hard, they have always been. And when you finally find something you like, there is always that thought, in the back of your head - "can I find something better?".

One of the challenges that arise in _Reinforcement Learning_ is the _Exploration - Exploitation Dilemma_, do we choose - **exploitation**, where we make the best decision given current information or **exploration**, where we gather more information that might lead us to better decisions in the future.

In this post we will introduce the Multi Armed Bandits (MAB) problem, show two algorithms that solve this problem, and run a simulation.

# Multi Armed Bandits

# Algorithms

## UCB

## Thompson Sampling

# Implementation

# Simulation

# Conclusion
