---
layout: post
title: Carrot and Stick
description: |
  A Framework to Learn Reinforcement Learning
image: /assets/img/Bandit-post/comics-doctor-octopus-day-off-shop.jpeg
noindex: true
---

A while ago I went with my bother to a Meetup about Reinforcement Learning (RL), 
I got into a conversation with some one that sat next to me, he asked me several question about the subject -
What is the difference between RL and supervised/unsupervised learning? 
What is the difference between several types of algorithms? 
When would you choose this framework over another one? 
I have to say, it took me a while to answer these questions, and not in the most precise manner.
On my way home I came to the conclusion that I really need to give my RL tool belt a spit shine.
And so, I started searching for a framework where I can easily implement RL algorithms/agents, 
test them in different scenarios, compare between them, but could not find anything that was a good fit.
All the frameworks that I found where either too complicated or lacked key features that I was looking for, 
and so it left me no choice but to build my own - _Carrot and Stick_.

In this post I wish to do several thing:
1. Answer the questions I was asked in the meetup
2. Introduce the _Carrot and Stick_ framework
3. A walk-through implementing the _Hill Climb algorithm_

Lets start with answering the questions

# What is Reinforcement Learning?
Reinforcement Learning is a framework for learning sequential decision making tasks. 
It is a machine learning paradigms, that differs from supervised learning and unsupervised learning in terms of goals. 

In a reinforcement learning problems, the goal is to find a good policy, 
an action for each state, maximizing some notion of cumulative reward.

In unsupervised learning the goal is to find similarities and differences between data-points. 

In supervised learning, similar to reinforcement learning, the goal is to maximize a "score" (accuracy, recall, ETC...).
However, because each decision is independent, we have a label associated with each decision, opposed to reinforcement learning, where labels are associated with sequences.

# What is this Framework
This framework aims to give you the ability to implement an agent, 
from scratch or by tweaking an existing one. 
Once the agent is ready, we can let him play a _game_ within a _world_ and test how well it performs.

In this framework, I have implemented several _agents_ and _worlds_ to be used as reference or as a benchmark.
In following posts I will pick a RL algorithm and show how to implement it in this framework.

# Modules in the Framework
The _Carrot and Stick_ framework is comprised of four different modules. 
Each module holds a base class with several methods and properties.
In this section I will go over each of the modules, explain how are they used, and elaborate on their base classes.

## World
## Agents
## Decision Models
## Games

# Example - Hill climb 
## Algorithm 
## Implementation 
## Results

# Conclusion and next Steps


